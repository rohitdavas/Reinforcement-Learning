{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T16:26:47.246742Z",
     "start_time": "2020-04-02T16:26:47.243673Z"
    }
   },
   "outputs": [],
   "source": [
    "''' imports '''\n",
    "\n",
    "import gym\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "''' model imports'''\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "''' data imports'''\n",
    "from collections import namedtuple \n",
    "\n",
    "''' visualisation '''\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Gym Concept and analysis \n",
    "\n",
    "    - observation space  Box(4,) \n",
    "    - action space:  Discrete(2) \n",
    "    - sample observation:  [-0.00220126 -0.01694481  0.01935941 -0.03202244]\n",
    "    - high:  3.4028235e+38\n",
    "    - low:  -3.4028235e+38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building The solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**issue**:\n",
    "\n",
    "    - our model was not imporving due to several reasons, which are hence the limitations of crossentropy method also. \n",
    "        - episodes should have been finite and preferably short.\n",
    "        - the total reward for the episodes should have enough variability to seprate good episodes from bad\n",
    "        - there is no intermediate indication about whether the agent has succedded or failed\n",
    "    \n",
    "**solution: clearly crooentropy method is not the good solution for it, but we can improve over croosentropy using some fixes**\n",
    "    - larger batches of played episodes\n",
    "    - discount factor applied to reward\n",
    "    - keeping elite episodes for a longer time\n",
    "    - decrease learning rate\n",
    "    - much longer training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T16:26:50.072104Z",
     "start_time": "2020-04-02T16:26:50.069687Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "lr = 0.001 \n",
    "percentile = 30\n",
    "Gamma = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T16:26:51.051196Z",
     "start_time": "2020-04-02T16:26:51.036743Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T16:26:55.193248Z",
     "start_time": "2020-04-02T16:26:54.992976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space:  Box(16,)\n",
      "action space:  Discrete(4)\n",
      "sample observation:  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "\n",
    "print(\"observation space: \", env.observation_space)\n",
    "print(\"action space: \", env.action_space)\n",
    "print(\"sample observation: \", env.reset())\n",
    "\n",
    "\n",
    "n_obs = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T16:26:59.377048Z",
     "start_time": "2020-04-02T16:26:59.369722Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (pipe): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden = 128 \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_obs, hidden, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.pipe = nn.Sequential(nn.Linear(in_features= n_obs, out_features= hidden),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(in_features= hidden, out_features= n_actions))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pipe(x) \n",
    "    \n",
    "net = Net(n_obs, hidden, n_actions)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T16:27:04.482568Z",
     "start_time": "2020-04-02T16:27:04.480061Z"
    }
   },
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names= (\"reward\", \"steps\"))\n",
    "Episode_step = namedtuple('Episode_step', field_names= (\"observation\", \"action\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T16:27:04.966891Z",
     "start_time": "2020-04-02T16:27:04.948222Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    single_episode = [] \n",
    "    \n",
    "    sm = nn.Softmax(dim= 1)\n",
    "    obs_ = env.reset()\n",
    "    \n",
    "    episode_reward = 0.0 \n",
    "    while True:\n",
    "        # obs -> get probs -> take action based on probs\n",
    "        obs = torch.FloatTensor([obs_])\n",
    "        action_probs = sm(net(obs)).data.numpy()[0]\n",
    "        action = np.random.choice(a = n_actions, p = action_probs)\n",
    "        \n",
    "        # single step in episode completes, append to the single_episode as an Episode Step \n",
    "        single_episode.append(Episode_step(observation= obs_, action= action)) \n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        episode_reward += reward \n",
    "        \n",
    "        if done:\n",
    "            # episode done, so make an episode tuple and append to the batch \n",
    "            batch.append(Episode(reward= episode_reward, steps= single_episode.copy()))\n",
    "            # clear for next episode \n",
    "            episode_reward = 0.0 \n",
    "            single_episode.clear()\n",
    "            new_obs = env.reset()\n",
    "            \n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch.clear() \n",
    "                \n",
    "        obs_ = new_obs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T16:30:02.502597Z",
     "start_time": "2020-04-02T16:30:02.491434Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def filterBatches(batch, percentile):\n",
    "    \n",
    "    ############### using discounted rewards ###########################\n",
    "    rewards = list(map(lambda s: s.reward*(Gamma**len(s.steps)), batch))\n",
    "    rewards_bound = np.percentile(a = rewards, q= percentile)\n",
    "    \n",
    "    train_obs = []\n",
    "    train_action = []\n",
    "    elite_batch = []\n",
    "                   \n",
    "    for episode, disc_reward in zip(batch, rewards):\n",
    "        if episode.reward >= rewards_bound:\n",
    "            observations = list(map(lambda s: s.observation, episode.steps))\n",
    "            actions = list(map(lambda s: s.action, episode.steps))\n",
    "            \n",
    "            train_obs.extend(observations)\n",
    "            train_action.extend(actions) \n",
    "            elite_batch.append(episode)\n",
    "\n",
    "       \n",
    "    train_obs = torch.FloatTensor(train_obs)\n",
    "    train_action = torch.LongTensor(train_action)\n",
    "    return train_obs, train_action,elite_batch, rewards_bound \n",
    "    \n",
    "    \n",
    "# for batch in iterate_batches(env, net, 3):\n",
    "#     break \n",
    "    \n",
    "# o, a, m, b = filterBatches(batch, 70)\n",
    "# print(o.shape, a.shape, m, b)\n",
    "\n",
    "# # torch.Size([7, 16]) torch.Size([7]) 0.0 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T16:27:09.677711Z",
     "start_time": "2020-04-02T16:27:09.670084Z"
    }
   },
   "outputs": [],
   "source": [
    "logs = gym.logger \n",
    "logs.set_level(gym.logger.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T16:39:51.673472Z",
     "start_time": "2020-04-02T16:39:51.668167Z"
    }
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment= 'extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T16:27:25.181422Z",
     "start_time": "2020-04-02T16:27:25.178459Z"
    }
   },
   "outputs": [],
   "source": [
    "objective = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(params= net.parameters(), lr= 0.001, betas= (0.9,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T17:25:54.449141Z",
     "start_time": "2020-04-02T16:40:20.070983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: loss: 1.373 mean_reward:   0 bound_reward: 0.000 0\n",
      "INFO: loss: 1.370 mean_reward:   0 bound_reward: 0.000 100\n",
      "INFO: loss: 1.370 mean_reward:   0 bound_reward: 0.000 200\n",
      "INFO: loss: 1.370 mean_reward:   0 bound_reward: 0.000 300\n",
      "INFO: loss: 1.371 mean_reward:   0 bound_reward: 0.000 400\n",
      "INFO: loss: 1.372 mean_reward:   0 bound_reward: 0.000 500\n",
      "INFO: loss: 1.371 mean_reward:   0 bound_reward: 0.000 600\n",
      "INFO: loss: 1.371 mean_reward:   0 bound_reward: 0.000 700\n",
      "INFO: loss: 1.373 mean_reward:   0 bound_reward: 0.000 800\n",
      "INFO: loss: 1.370 mean_reward:   0 bound_reward: 0.000 900\n",
      "INFO: loss: 1.369 mean_reward:   0 bound_reward: 0.000 1000\n",
      "INFO: loss: 1.368 mean_reward:   0 bound_reward: 0.000 1100\n",
      "INFO: loss: 1.367 mean_reward:   0 bound_reward: 0.000 1200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-005f51b13d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mr_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# get data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melite_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilterBatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melite_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0melite_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-5d7b869b543c>\u001b[0m in \u001b[0;36mfilterBatches\u001b[0;34m(batch, percentile)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_action\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melite_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_bound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' main script '''\n",
    "elite_batches = []\n",
    "for i, batch in enumerate(iterate_batches(env, net, batch_size)):\n",
    "    r_mean = float(np.mean(list(map(lambda s:s.reward, batch))))\n",
    "    # get data\n",
    "    obs, actions, elite_batches, r_bound = filterBatches(elite_batches + batch, percentile= percentile)\n",
    "    \n",
    "    if not elite_batches:\n",
    "        continue \n",
    "    \n",
    "    \n",
    "    ############################# train ##################################\n",
    "    # set gradients zero \n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # forward\n",
    "    logits = net(obs)\n",
    "    # loss\n",
    "    loss = objective(logits, actions)\n",
    "    # gradients\n",
    "    loss.backward()\n",
    "    # optimize\n",
    "    opt.step()\n",
    "    \n",
    "    ############################ writer ####################################\n",
    "    if i % 100 == 0:\n",
    "        logs.info(\"loss: %.3f mean_reward: %3.f bound_reward: %.3f %d\",loss.item(), r_mean, r_bound, i)\n",
    "        writer.add_scalar('mean_reward: ', r_mean, i)\n",
    "        writer.add_scalar('loss', loss.item(), i)\n",
    "        writer.add_scalar('bound_reward', r_bound, i)\n",
    "\n",
    "    if r_mean > 0.8:\n",
    "        print(\"solved with mean of \", r_mean)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<img src=\"./bound_reward_naive curve.svg\" style = \"width:400px;\"> Reward bound </img>**\n",
    " > reward bound is not improving, indicationg percentile = 70 th number episode is still zero.\n",
    " \n",
    "**<img src=\"./loss_naive curve.svg\" style = \"width:600px;\" > loss curve </img>**\n",
    "**<img src=\"./mean_reward_naive curve.svg\" style = \"width:400px;\" > mean reward curve</img>**\n",
    "   >mean reward near to zero and not improving much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
