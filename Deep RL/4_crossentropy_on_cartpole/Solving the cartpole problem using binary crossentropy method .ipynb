{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:11:06.338954Z",
     "start_time": "2020-04-02T07:11:05.615755Z"
    }
   },
   "outputs": [],
   "source": [
    "''' model imports'''\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import numpy as np \n",
    "\n",
    "''' data imports'''\n",
    "from collections import namedtuple \n",
    "import gym \n",
    "\n",
    "\n",
    "\n",
    "''' print related options'''\n",
    "from pprint import pprint \n",
    "\n",
    "''' writer '''\n",
    "from tensorboardX import SummaryWriter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**cartpole environment**\n",
    "     - provides observation as 4 value tuple\n",
    "     - provides an option of taking 2 actions over the returned value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:14:30.498998Z",
     "start_time": "2020-04-02T07:14:30.365374Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: xvfb-run: not found\n",
      "action space \t Discrete(2)\n",
      "observation space \t Box(4,)\n",
      "sample observation \t [-0.04157752 -0.00740001 -0.01385926 -0.03295999]\n",
      "reward range \t (-inf, inf)\n",
      "no of actions =  2\n",
      "no of obs =  4\n"
     ]
    }
   ],
   "source": [
    "''' analyse and create the environment '''\n",
    "gym.envs\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "env = gym.wrappers.Monitor(env, directory= \"mon\", force= True, video_callable= False)\n",
    "!xvfb-run -s \"-screen 0 640*480*24\"\n",
    "print(\"action space \\t\", env.action_space)\n",
    "print(\"observation space \\t\", env.observation_space)\n",
    "print(\"sample observation \\t\", env.reset())\n",
    "print(\"reward range \\t\", env.reward_range)\n",
    "# print(\"env \", env.env)\n",
    "# print(\"spec \", env.spec)\n",
    "# print(\"unwrapped \", env.unwrapped)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "n_obs = len(env.reset())\n",
    "n_obs = env.observation_space.shape[0] # can be calculated by any of this way \n",
    "\n",
    "print(\"no of actions = \", n_actions)\n",
    "print(\"no of obs = \", n_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "useful python functions:\n",
    " - [lambda function](https://www.programiz.com/python-programming/anonymous-function) \n",
    " - [map](https://www.programiz.com/python-programming/methods/built-in/map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:09:29.128472Z",
     "start_time": "2020-04-02T07:09:29.114624Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "''' define the model '''\n",
    "''' n_obs -> Hidden -> n_actions '''\n",
    "\n",
    "Hidden = 128 \n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_obs, hidden, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.denseNet = nn.Sequential(nn.Linear(in_features= n_obs, out_features= hidden),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(in_features= hidden, out_features= n_actions))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.denseNet(x) \n",
    "        \n",
    "net = Net(n_obs,Hidden, n_actions)\n",
    "print(net)\n",
    "\n",
    "# if needed to load state dict for pretrained model \n",
    "# net.load_state_dict(torch.load('./model/cartpoleNet.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T06:55:52.283061Z",
     "start_time": "2020-04-02T06:55:52.278725Z"
    }
   },
   "outputs": [],
   "source": [
    "''' Define storage container '''\n",
    "\n",
    "Episode = namedtuple(typename= 'Episode', field_names= ['reward', 'steps'])\n",
    "EpisodeStep = namedtuple(typename= 'EpisodeStep', field_names= ['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T06:56:16.459587Z",
     "start_time": "2020-04-02T06:56:16.445871Z"
    },
    "code_folding": [
     3,
     40
    ],
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' generate data '''\n",
    "batch_size = 16\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_steps = [] \n",
    "    episode_reward = 0.0 \n",
    "    \n",
    "    obs = env.reset() # list return \n",
    "    sm = nn.Softmax(dim= 1 )\n",
    "    \n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs]) # torch accepts in format [batch_size, n_obs]\n",
    "        net_output = sm(net(obs_v))\n",
    "        \n",
    "        action_probs = net_output.data.numpy()[0]\n",
    "        action = np.random.choice(a = len(action_probs), p = action_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        \n",
    "        episode_reward += reward \n",
    "        episode_steps.append(EpisodeStep(observation= obs, action= action))\n",
    "        \n",
    "        if is_done:\n",
    "            batch.append(Episode(reward= episode_reward, steps= episode_steps))\n",
    "            \n",
    "            episode_reward = 0.0 \n",
    "            episode_steps = []\n",
    "            \n",
    "            next_obs = env.reset()\n",
    "            \n",
    "            if len(batch) == batch_size:\n",
    "                yield batch \n",
    "                batch = []\n",
    "                \n",
    "                \n",
    "        obs = next_obs \n",
    "        \n",
    "        \n",
    "################### filter batches \n",
    "\n",
    "def filterBatch(batch:list, percentile:int):\n",
    "    train_obs = []\n",
    "    train_actions = []\n",
    "    \n",
    "    rewards = list(map(lambda s:s.reward, batch))\n",
    "    rewards_bound = np.percentile(rewards, percentile)\n",
    "    rewards_mean = np.mean(rewards) \n",
    "    \n",
    "    for episode in batch:\n",
    "        if episode.reward < rewards_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step:step.observation, episode.steps))\n",
    "        train_actions.extend(map(lambda step: step.action, episode.steps))\n",
    "    \n",
    "    train_obs = torch.FloatTensor(train_obs)\n",
    "    train_actions = torch.LongTensor(train_actions)\n",
    "    \n",
    "    return train_obs, train_actions, rewards_bound, rewards_mean \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T06:56:18.733097Z",
     "start_time": "2020-04-02T06:56:18.729611Z"
    }
   },
   "outputs": [],
   "source": [
    "log = gym.logger\n",
    "log.set_level(gym.logger.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T06:56:45.223214Z",
     "start_time": "2020-04-02T06:56:45.219301Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer \n",
    "opt = optim.Adam(net.parameters(), lr= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:10:43.198980Z",
     "start_time": "2020-04-02T07:10:43.177422Z"
    }
   },
   "outputs": [],
   "source": [
    "''' main script '''\n",
    "writer = SummaryWriter() \n",
    "env.reset()\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, batch_size)):\n",
    "    train_obs, train_actions, rewards_bound, rewards_mean = filterBatch(batch, 50)\n",
    "    \n",
    "    ################# train #################################\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    action_scores = net(train_obs)\n",
    "    \n",
    "    loss = objective(action_scores, train_actions)\n",
    "    loss.backward()\n",
    "    opt.step() \n",
    "\n",
    "    ############### write to tensorboard ####################\n",
    "    log.info(\"Iter %d: loss=%.3f, mean reward =%.3f, reward bound = %.3f\", iter_no,loss.item(),rewards_mean, rewards_bound)\n",
    "    writer.add_scalar(\"loss\", loss.item(), iter_no)\n",
    "    writer.add_scalar(\"mean_reward\", rewards_mean, iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", rewards_bound, iter_no)\n",
    "    \n",
    "    if rewards_mean > 199:\n",
    "        print(rewards_mean)\n",
    "        print(\"solved\")\n",
    "        print(\"saving model weights \")\n",
    "        torch.save(net.state_dict(), f= './model/cartpoleNet.pt')\n",
    "        break \n",
    "        \n",
    "    writer.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:08:51.744054Z",
     "start_time": "2020-04-02T07:08:47.885299Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(env, net):\n",
    "    episode_reward = 0.0 \n",
    "    obs = env.reset() # list return \n",
    "    sm = nn.Softmax(dim= 1 )\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs]) # torch accepts in format [batch_size, n_obs]\n",
    "        net_output = sm(net(obs_v))\n",
    "        \n",
    "        action_probs = net_output.data.numpy()[0]\n",
    "        action = np.random.choice(a = len(action_probs), p = action_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        \n",
    "        episode_reward += reward \n",
    "        \n",
    "        if is_done:\n",
    "            print(\"episode reward: \", episode_reward)\n",
    "            rewards.append(episode_reward)\n",
    "            \n",
    "            episode_reward = 0.0 \n",
    "            next_obs = env.reset()\n",
    "            \n",
    "            if len(rewards) == 100:\n",
    "                print(\"mean reward of hundred episodes \", np.mean(rewards))\n",
    "                break \n",
    "                \n",
    "                \n",
    "                \n",
    "        obs = next_obs \n",
    "        \n",
    "test(env, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
