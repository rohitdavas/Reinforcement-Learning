{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:03:51.074863Z",
     "start_time": "2020-04-01T11:03:51.071224Z"
    }
   },
   "outputs": [],
   "source": [
    "''' for data generation '''\n",
    "\n",
    "import gym\n",
    "import gym.spaces \n",
    "import cv2\n",
    "import numpy as np \n",
    "import random\n",
    "import torch \n",
    "\n",
    "\n",
    "''' model '''\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "\n",
    "''' summary writer '''\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "''' imagee '''\n",
    "import torchvision.utils as vutils "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataSource: atari Environment** \n",
    "***\n",
    "Changes we need to make: \n",
    "     - atari Image resolution: 210 * 160  => 64 * 64 \n",
    "     - atari image format: channel last => pytorch channel type channel first \n",
    "     - data type, image has uint8 => calculation needs float32 \n",
    "\n",
    "***\n",
    "how to make these changes: \n",
    "\n",
    "There are two ways for it: \n",
    "\n",
    "    First - making changes directly to gym env by using InputWrapper\n",
    "        - create a Input wrapper inheriting the gym observation wrapper\n",
    "        - change the observation space to Box => but why? I don't get it. \n",
    "        \n",
    "    Second: get the observation from gym, postprocess it accordingly. \n",
    "        - get obs using env.step\n",
    "        - define a function to modify it. \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:02:16.039353Z",
     "start_time": "2020-04-01T10:02:16.036282Z"
    }
   },
   "outputs": [],
   "source": [
    "''' constants we need '''\n",
    "Image_size = 64 # output image size for our GAN \n",
    "batch_size = 16  # batch size to generate from env \n",
    "\n",
    "\n",
    "# saving env images to disk \n",
    "saved_index = 0 \n",
    "max_save = 100 \n",
    "save = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  method 1: inputwrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:22:09.621214Z",
     "start_time": "2020-04-01T10:22:09.611400Z"
    },
    "code_folding": [
     0,
     12,
     19
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class InputWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, *args):\n",
    "        super(InputWrapper, self).__init__(*args)\n",
    "        assert isinstance(self.observation_space, gym.spaces.Box)\n",
    "        old_space = self.observation_space\n",
    "        self.observation_space = gym.spaces.Box(self.observation(old_space.low), self.observation(old_space.high),\n",
    "                                                dtype=np.float32)\n",
    "           \n",
    "    def observation(self, observation):\n",
    "        global save \n",
    "        new_obs = cv2.resize(observation, (Image_size, Image_size)) \n",
    "        \n",
    "        if save and np.mean(new_obs) > 0.01:\n",
    "            self.save_images(new_obs)\n",
    "            \n",
    "        new_obs = np.moveaxis(a = new_obs, source= 2,destination= 0)\n",
    "        new_obs = new_obs.astype(np.float32) \n",
    "        return new_obs    \n",
    "    \n",
    "    def save_images(self, obs):\n",
    "        global saved_index , max_save \n",
    "        if saved_index < max_save :\n",
    "            cv2.imwrite( './atari saved images/wrapper_method/img' + str(saved_index) + '.png', np.uint8(obs))\n",
    "        saved_index += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:22:34.732689Z",
     "start_time": "2020-04-01T10:22:34.726164Z"
    },
    "code_folding": [
     0,
     32
    ]
   },
   "outputs": [],
   "source": [
    "def iterate_batches(envs):\n",
    "    \n",
    "    global saved_index\n",
    "    initial_images_of_env = [e.reset() for e in envs] \n",
    "    \n",
    "    batch = []\n",
    "    \n",
    "    # select a random environment from envs \n",
    "    env_gen = iter(lambda: random.choice(envs), None)\n",
    "\n",
    "    while True:\n",
    "        e = next(env_gen)\n",
    "        obs, reward, is_done, _ = e.step(e.action_space.sample())\n",
    "\n",
    "        if np.mean(obs) > 0.01:\n",
    "            batch.append(obs)\n",
    "        \n",
    "        if len(batch) == batch_size:\n",
    "            batch_np = np.asarray(batch, np.float32) * 2 / 255.0 - 1\n",
    "            yield torch.tensor(batch_np)\n",
    "            batch.clear()\n",
    "\n",
    "        if is_done:\n",
    "            e.reset()  \n",
    "            \n",
    "            \n",
    "  \n",
    "\n",
    "      \n",
    "# env_names = ['Breakout-v0', 'AirRaid-v0', 'Pong-v0']\n",
    "\n",
    "# envs = [InputWrapper(gym.make(name)) for name in env_names] \n",
    "# for e in envs:\n",
    "#     print(e.observation_space.shape)\n",
    "    \n",
    "# x_max = 1\n",
    "# x = 0 \n",
    "\n",
    "# for batch_v in iterate_batches(envs):\n",
    "#     if x < x_max:\n",
    "#         x+= 1 \n",
    "#         print(batch_v.size())\n",
    "#         continue \n",
    "#     else:\n",
    "#         break \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## method 2: define these operation outside of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:00:24.014884Z",
     "start_time": "2020-04-01T10:00:24.007663Z"
    },
    "code_folding": [
     11
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ''' constants we need '''\n",
    "\n",
    "# Image_size = 64 # output image size for our GAN \n",
    "# batch_size = 16  # batch size to generate from env \n",
    "\n",
    "# # saving env images to disk \n",
    "# save = True \n",
    "# saved_index = 0 \n",
    "# max_save = 100 \n",
    "\n",
    "\n",
    "# def save_image(obs):\n",
    "#     global saved_index \n",
    "#     if saved_index < max_save:\n",
    "#         cv2.imwrite(\n",
    "#             './atari saved images/non_wrapper_method/img' + str(saved_index) + '.png',\n",
    "#             np.uint8(obs))\n",
    "#         saved_index += 1\n",
    "\n",
    "\n",
    "# def preprocess(obs):\n",
    "#     obs = cv2.resize(obs, (Image_size, Image_size))\n",
    "#     if save and saved_index < max_save:\n",
    "#         save_image(obs) \n",
    "        \n",
    "#     obs = np.moveaxis(a=obs, source=2, destination=0)\n",
    "#     obs = obs.astype(np.float32)\n",
    "#     return obs\n",
    "\n",
    "\n",
    "# def iterate_batches(envs):\n",
    "#     global saved_index, save, batch_size\n",
    "    \n",
    "#     [e.reset() for e in envs] \n",
    "    \n",
    "#     batch = []\n",
    "#     env_gen = iter(lambda: random.choice(envs), None)\n",
    "\n",
    "#     while True:\n",
    "#         e = next(env_gen)\n",
    "#         obs, reward, is_done, _ = e.step(e.action_space.sample())\n",
    "        \n",
    "#         # check for non-zero mean of image, due to bug in one of the games to prevent flickering of images\n",
    "        \n",
    "#         if np.mean(obs) > 0.01:\n",
    "#             obs = preprocess(obs)\n",
    "#             batch.append(obs)\n",
    "        \n",
    "#         if len(batch) == batch_size:\n",
    "#             batch_np = np.asarray(batch, np.float32) * 2 / 255.0 - 1 # domain to -1 to 1 \n",
    "#             yield torch.tensor(batch_np)\n",
    "#             batch.clear()\n",
    "\n",
    "#         if is_done:\n",
    "#             e.reset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:00:24.648159Z",
     "start_time": "2020-04-01T10:00:24.634995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# env_names = ['Breakout-v0', 'AirRaid-v0', 'Pong-v0']\n",
    "\n",
    "# envs = [gym.make(name) for name in env_names] \n",
    "\n",
    "# x_max = 2 \n",
    "# x = 0 \n",
    "\n",
    "# for batch_v in iterate_batches(envs):\n",
    "#     if x < x_max:\n",
    "#         x+= 1 \n",
    "#         print(batch_v.size())\n",
    "#         continue \n",
    "#     else:\n",
    "#         break \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:07:29.479514Z",
     "start_time": "2020-04-01T10:07:29.477115Z"
    }
   },
   "outputs": [],
   "source": [
    "''' Discriminator constants '''\n",
    "DISC_FILTERS = 64 \n",
    "input_channels =  3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:07:31.505036Z",
     "start_time": "2020-04-01T10:07:31.496950Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv_pipe = nn.Sequential(\n",
    "            # 64 -> 32 \n",
    "            nn.Conv2d(in_channels= input_channels, out_channels= DISC_FILTERS,kernel_size= 4, stride= 2, padding= 1 ),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #32 -> 16\n",
    "            nn.Conv2d(in_channels= DISC_FILTERS, out_channels= DISC_FILTERS*2, kernel_size= 4, stride = 2, padding= 1),\n",
    "            nn.BatchNorm2d(DISC_FILTERS*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #16->8\n",
    "            nn.Conv2d(in_channels= DISC_FILTERS*2, out_channels= DISC_FILTERS*4, kernel_size=4, stride= 2, padding=1 ),\n",
    "            nn.BatchNorm2d(DISC_FILTERS*4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #8->4\n",
    "            nn.Conv2d(in_channels= DISC_FILTERS*4, out_channels= DISC_FILTERS*8, kernel_size= 4, stride= 2, padding = 1),\n",
    "            nn.BatchNorm2d(DISC_FILTERS*8),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #4->1 \n",
    "            nn.Conv2d(in_channels= DISC_FILTERS*8, out_channels= 1, kernel_size= 4, stride= 1, padding= 0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_pipe(x)\n",
    "        \n",
    "        #reshape \n",
    "        out = out.view(-1, 1).squeeze(dim = 1) \n",
    "        return out \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:22:07.240036Z",
     "start_time": "2020-04-01T09:22:07.184750Z"
    }
   },
   "outputs": [],
   "source": [
    "# '''test your discriminator '''\n",
    "# disc = Discriminator(input_channels)\n",
    "# test_output = disc(batch_v)\n",
    "# print(test_output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:07:50.465865Z",
     "start_time": "2020-04-01T10:07:50.463465Z"
    }
   },
   "outputs": [],
   "source": [
    "''' generator constants''' \n",
    "out_channels = 3 \n",
    "generator_filters = 64\n",
    "latent_vector_size = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:07:51.171615Z",
     "start_time": "2020-04-01T10:07:51.165263Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, out_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.deconvpipe = nn.Sequential(\n",
    "            # 4*4\n",
    "            nn.ConvTranspose2d(in_channels= latent_vector_size, out_channels = generator_filters*8,kernel_size= 4, stride= 1, padding= 0),\n",
    "            nn.BatchNorm2d(generator_filters*8),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 8*8\n",
    "            nn.ConvTranspose2d(in_channels= generator_filters*8, out_channels = generator_filters*4,kernel_size= 4, stride= 2, padding= 1),\n",
    "            nn.BatchNorm2d(generator_filters*4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 16*16\n",
    "            nn.ConvTranspose2d(in_channels= generator_filters*4, out_channels = generator_filters*2, kernel_size= 4, stride= 2, padding=1),\n",
    "            nn.BatchNorm2d(generator_filters*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 32*32 \n",
    "            nn.ConvTranspose2d(in_channels= generator_filters*2, out_channels = generator_filters, kernel_size=4, stride= 2, padding= 1),\n",
    "            nn.BatchNorm2d(generator_filters),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 64*64\n",
    "            nn.ConvTranspose2d(in_channels= generator_filters, out_channels = out_channels, kernel_size = 4, stride= 2, padding= 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.deconvpipe(x)\n",
    "        return out \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:08:28.543252Z",
     "start_time": "2020-04-01T10:08:28.541377Z"
    }
   },
   "outputs": [],
   "source": [
    "# gen = Generator(out_channels)\n",
    "# test_in = torch.FloatTensor(1, latent_vector_size, 1, 1).normal_(0,1) \n",
    "# test_out = gen(test_in)\n",
    "# print(gen)\n",
    "\n",
    "# print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:11:21.797750Z",
     "start_time": "2020-04-01T10:11:19.136689Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device:  cuda\n",
      "Generator(\n",
      "  (deconvpipe): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU()\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (conv_pipe): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' main script '''\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"used device: \", device)\n",
    "\n",
    "gen = Generator(out_channels).to(device)\n",
    "disc = Discriminator(input_channels).to(device)\n",
    "\n",
    "print(gen)\n",
    "print(disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:22:51.991346Z",
     "start_time": "2020-04-01T10:22:51.562794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "env_names = ['Breakout-v0', 'AirRaid-v0', 'Pong-v0']\n",
    "envs = [InputWrapper(gym.make(name)) for name in env_names] \n",
    "\n",
    "print(\"input shape: \", envs[0].observation_space.shape)\n",
    "\n",
    "\n",
    "objective = nn.BCELoss()\n",
    "gopt = optim.Adam(params= gen.parameters(), lr= 0.0001, betas= (0.5, 0.999))\n",
    "dopt = optim.Adam(params= disc.parameters(), lr = 0.0001, betas= (0.5, 0.999))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:58:43.356751Z",
     "start_time": "2020-04-01T10:58:43.349843Z"
    }
   },
   "outputs": [],
   "source": [
    "log = gym.logger\n",
    "log.set_level(gym.logger.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:55:00.244212Z",
     "start_time": "2020-04-01T13:34:54.746853Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Iter 100: gen_loss=5.884e+00, dis_loss=4.332e-02\n",
      "INFO: Iter 200: gen_loss=6.145e+00, dis_loss=1.291e-01\n",
      "INFO: Iter 300: gen_loss=6.171e+00, dis_loss=6.188e-02\n",
      "INFO: Iter 400: gen_loss=8.798e+00, dis_loss=1.665e-01\n",
      "INFO: Iter 500: gen_loss=7.023e+00, dis_loss=1.476e-01\n",
      "INFO: Iter 600: gen_loss=7.588e+00, dis_loss=9.536e-02\n",
      "INFO: Iter 700: gen_loss=8.383e+00, dis_loss=7.884e-02\n",
      "INFO: Iter 800: gen_loss=6.294e+00, dis_loss=1.509e-01\n",
      "INFO: Iter 900: gen_loss=7.889e+00, dis_loss=9.599e-02\n",
      "INFO: Iter 1000: gen_loss=8.300e+00, dis_loss=2.501e-02\n",
      "INFO: Iter 1100: gen_loss=8.669e+00, dis_loss=1.303e-01\n",
      "INFO: Iter 1200: gen_loss=7.521e+00, dis_loss=1.525e-02\n",
      "INFO: Iter 1300: gen_loss=7.671e+00, dis_loss=2.952e-02\n",
      "INFO: Iter 1400: gen_loss=9.336e+00, dis_loss=7.092e-02\n",
      "INFO: Iter 1500: gen_loss=8.557e+00, dis_loss=2.447e-01\n",
      "INFO: Iter 1600: gen_loss=8.790e+00, dis_loss=2.840e-02\n",
      "INFO: Iter 1700: gen_loss=8.849e+00, dis_loss=3.918e-02\n",
      "INFO: Iter 1800: gen_loss=6.843e+00, dis_loss=5.281e-02\n",
      "INFO: Iter 1900: gen_loss=8.318e+00, dis_loss=1.633e-01\n",
      "INFO: Iter 2000: gen_loss=8.260e+00, dis_loss=1.136e-01\n",
      "INFO: Iter 2100: gen_loss=7.841e+00, dis_loss=2.143e-01\n",
      "INFO: Iter 2200: gen_loss=7.505e+00, dis_loss=7.167e-02\n",
      "INFO: Iter 2300: gen_loss=8.132e+00, dis_loss=4.807e-02\n",
      "INFO: Iter 2400: gen_loss=7.664e+00, dis_loss=1.538e-01\n",
      "INFO: Iter 2500: gen_loss=8.134e+00, dis_loss=1.517e-01\n",
      "INFO: Iter 2600: gen_loss=7.980e+00, dis_loss=8.537e-02\n",
      "INFO: Iter 2700: gen_loss=8.227e+00, dis_loss=8.969e-02\n",
      "INFO: Iter 2800: gen_loss=8.750e+00, dis_loss=8.060e-02\n",
      "INFO: Iter 2900: gen_loss=7.859e+00, dis_loss=1.724e-01\n",
      "INFO: Iter 3000: gen_loss=7.690e+00, dis_loss=1.328e-01\n",
      "INFO: Iter 3100: gen_loss=7.697e+00, dis_loss=9.115e-02\n",
      "INFO: Iter 3200: gen_loss=8.213e+00, dis_loss=1.920e-01\n",
      "INFO: Iter 3300: gen_loss=7.155e+00, dis_loss=8.077e-02\n",
      "INFO: Iter 3400: gen_loss=7.440e+00, dis_loss=6.836e-02\n",
      "INFO: Iter 3500: gen_loss=7.915e+00, dis_loss=6.596e-02\n",
      "INFO: Iter 3600: gen_loss=9.039e+00, dis_loss=3.927e-02\n",
      "INFO: Iter 3700: gen_loss=5.923e+00, dis_loss=3.189e-01\n",
      "INFO: Iter 3800: gen_loss=7.258e+00, dis_loss=1.134e-01\n",
      "INFO: Iter 3900: gen_loss=7.285e+00, dis_loss=1.000e-01\n",
      "INFO: Iter 4000: gen_loss=7.127e+00, dis_loss=1.128e-01\n",
      "INFO: Iter 4100: gen_loss=6.405e+00, dis_loss=6.262e-02\n",
      "INFO: Iter 4200: gen_loss=6.947e+00, dis_loss=8.993e-02\n",
      "INFO: Iter 4300: gen_loss=8.411e+00, dis_loss=1.493e-01\n",
      "INFO: Iter 4400: gen_loss=7.495e+00, dis_loss=1.055e-01\n",
      "INFO: Iter 4500: gen_loss=7.282e+00, dis_loss=9.486e-02\n",
      "INFO: Iter 4600: gen_loss=8.170e+00, dis_loss=6.450e-02\n",
      "INFO: Iter 4700: gen_loss=7.468e+00, dis_loss=4.179e-02\n",
      "INFO: Iter 4800: gen_loss=8.228e+00, dis_loss=1.035e-01\n",
      "INFO: Iter 4900: gen_loss=7.928e+00, dis_loss=3.807e-02\n",
      "INFO: Iter 5000: gen_loss=8.779e+00, dis_loss=1.424e-02\n",
      "INFO: Iter 5100: gen_loss=9.556e+00, dis_loss=1.467e-02\n",
      "INFO: Iter 5200: gen_loss=1.030e+01, dis_loss=1.364e-02\n",
      "INFO: Iter 5300: gen_loss=6.914e+00, dis_loss=1.598e-01\n",
      "INFO: Iter 5400: gen_loss=6.768e+00, dis_loss=2.865e-01\n",
      "INFO: Iter 5500: gen_loss=6.095e+00, dis_loss=4.450e-02\n",
      "INFO: Iter 5600: gen_loss=7.636e+00, dis_loss=1.193e-01\n",
      "INFO: Iter 5700: gen_loss=7.900e+00, dis_loss=8.706e-02\n",
      "INFO: Iter 5800: gen_loss=7.333e+00, dis_loss=9.371e-02\n",
      "INFO: Iter 5900: gen_loss=7.638e+00, dis_loss=2.778e-02\n",
      "INFO: Iter 6000: gen_loss=7.975e+00, dis_loss=3.827e-02\n",
      "INFO: Iter 6100: gen_loss=8.052e+00, dis_loss=6.655e-02\n",
      "INFO: Iter 6200: gen_loss=8.287e+00, dis_loss=2.049e-02\n",
      "INFO: Iter 6300: gen_loss=7.839e+00, dis_loss=1.740e-01\n",
      "INFO: Iter 6400: gen_loss=7.241e+00, dis_loss=1.322e-01\n",
      "INFO: Iter 6500: gen_loss=6.343e+00, dis_loss=1.808e-01\n",
      "INFO: Iter 6600: gen_loss=7.123e+00, dis_loss=5.092e-02\n",
      "INFO: Iter 6700: gen_loss=7.112e+00, dis_loss=3.997e-02\n",
      "INFO: Iter 6800: gen_loss=8.245e+00, dis_loss=3.614e-02\n",
      "INFO: Iter 6900: gen_loss=8.289e+00, dis_loss=1.590e-01\n",
      "INFO: Iter 7000: gen_loss=8.128e+00, dis_loss=3.842e-02\n",
      "INFO: Iter 7100: gen_loss=8.126e+00, dis_loss=9.643e-02\n",
      "INFO: Iter 7200: gen_loss=8.668e+00, dis_loss=3.941e-02\n",
      "INFO: Iter 7300: gen_loss=7.764e+00, dis_loss=6.056e-02\n",
      "INFO: Iter 7400: gen_loss=7.492e+00, dis_loss=1.065e-01\n",
      "INFO: Iter 7500: gen_loss=8.133e+00, dis_loss=8.581e-02\n",
      "INFO: Iter 7600: gen_loss=7.792e+00, dis_loss=9.361e-02\n",
      "INFO: Iter 7700: gen_loss=8.373e+00, dis_loss=1.852e-02\n",
      "INFO: Iter 7800: gen_loss=7.545e+00, dis_loss=6.630e-02\n",
      "INFO: Iter 7900: gen_loss=7.408e+00, dis_loss=1.302e-01\n",
      "INFO: Iter 8000: gen_loss=7.473e+00, dis_loss=3.204e-02\n",
      "INFO: Iter 8100: gen_loss=9.170e+00, dis_loss=1.259e-01\n",
      "INFO: Iter 8200: gen_loss=7.141e+00, dis_loss=9.802e-02\n",
      "INFO: Iter 8300: gen_loss=7.614e+00, dis_loss=3.352e-02\n",
      "INFO: Iter 8400: gen_loss=7.055e+00, dis_loss=1.361e-02\n",
      "INFO: Iter 8500: gen_loss=8.022e+00, dis_loss=1.314e-01\n",
      "INFO: Iter 8600: gen_loss=6.565e+00, dis_loss=1.568e-01\n",
      "INFO: Iter 8700: gen_loss=8.665e+00, dis_loss=9.443e-02\n",
      "INFO: Iter 8800: gen_loss=8.871e+00, dis_loss=1.266e-02\n",
      "INFO: Iter 8900: gen_loss=8.660e+00, dis_loss=3.297e-02\n",
      "INFO: Iter 9000: gen_loss=8.169e+00, dis_loss=5.076e-02\n",
      "INFO: Iter 9100: gen_loss=8.085e+00, dis_loss=2.058e-01\n",
      "INFO: Iter 9200: gen_loss=6.686e+00, dis_loss=8.199e-02\n",
      "INFO: Iter 9300: gen_loss=6.770e+00, dis_loss=6.162e-02\n",
      "INFO: Iter 9400: gen_loss=7.688e+00, dis_loss=1.945e-01\n",
      "INFO: Iter 9500: gen_loss=8.630e+00, dis_loss=1.118e-01\n",
      "INFO: Iter 9600: gen_loss=7.477e+00, dis_loss=5.488e-02\n",
      "INFO: Iter 9700: gen_loss=7.465e+00, dis_loss=1.435e-01\n",
      "INFO: Iter 9800: gen_loss=7.252e+00, dis_loss=1.050e-01\n",
      "INFO: Iter 9900: gen_loss=7.493e+00, dis_loss=3.324e-02\n",
      "INFO: Iter 10000: gen_loss=7.341e+00, dis_loss=1.607e-01\n",
      "INFO: Iter 10100: gen_loss=8.253e+00, dis_loss=4.365e-02\n",
      "INFO: Iter 10200: gen_loss=8.572e+00, dis_loss=4.165e-02\n",
      "INFO: Iter 10300: gen_loss=7.602e+00, dis_loss=1.513e-01\n",
      "INFO: Iter 10400: gen_loss=7.741e+00, dis_loss=9.069e-02\n",
      "INFO: Iter 10500: gen_loss=7.936e+00, dis_loss=1.260e-01\n",
      "INFO: Iter 10600: gen_loss=8.555e+00, dis_loss=4.990e-02\n",
      "INFO: Iter 10700: gen_loss=7.932e+00, dis_loss=3.572e-02\n",
      "INFO: Iter 10800: gen_loss=9.036e+00, dis_loss=4.674e-02\n",
      "INFO: Iter 10900: gen_loss=7.511e+00, dis_loss=1.222e-01\n",
      "INFO: Iter 11000: gen_loss=7.992e+00, dis_loss=4.044e-02\n",
      "INFO: Iter 11100: gen_loss=8.283e+00, dis_loss=1.927e-02\n",
      "INFO: Iter 11200: gen_loss=7.452e+00, dis_loss=8.627e-02\n",
      "INFO: Iter 11300: gen_loss=8.780e+00, dis_loss=7.113e-02\n",
      "INFO: Iter 11400: gen_loss=8.707e+00, dis_loss=1.127e-01\n",
      "INFO: Iter 11500: gen_loss=7.203e+00, dis_loss=1.566e-01\n",
      "INFO: Iter 11600: gen_loss=8.731e+00, dis_loss=1.548e-02\n",
      "INFO: Iter 11700: gen_loss=8.884e+00, dis_loss=1.999e-01\n",
      "INFO: Iter 11800: gen_loss=8.113e+00, dis_loss=1.631e-02\n",
      "INFO: Iter 11900: gen_loss=7.651e+00, dis_loss=2.186e-01\n",
      "INFO: Iter 12000: gen_loss=5.760e+00, dis_loss=1.149e-01\n",
      "INFO: Iter 12100: gen_loss=6.758e+00, dis_loss=2.328e-01\n",
      "INFO: Iter 12200: gen_loss=7.540e+00, dis_loss=8.284e-02\n",
      "INFO: Iter 12300: gen_loss=7.446e+00, dis_loss=7.909e-02\n",
      "INFO: Iter 12400: gen_loss=7.098e+00, dis_loss=7.603e-02\n",
      "INFO: Iter 12500: gen_loss=7.499e+00, dis_loss=6.132e-02\n",
      "INFO: Iter 12600: gen_loss=7.836e+00, dis_loss=1.039e-01\n",
      "INFO: Iter 12700: gen_loss=6.257e+00, dis_loss=1.424e-01\n",
      "INFO: Iter 12800: gen_loss=7.040e+00, dis_loss=1.555e-01\n",
      "INFO: Iter 12900: gen_loss=6.951e+00, dis_loss=4.394e-02\n",
      "INFO: Iter 13000: gen_loss=7.272e+00, dis_loss=8.763e-02\n",
      "INFO: Iter 13100: gen_loss=7.961e+00, dis_loss=2.621e-02\n",
      "INFO: Iter 13200: gen_loss=7.518e+00, dis_loss=2.610e-01\n",
      "INFO: Iter 13300: gen_loss=6.824e+00, dis_loss=1.168e-01\n",
      "INFO: Iter 13400: gen_loss=7.002e+00, dis_loss=1.724e-01\n",
      "INFO: Iter 13500: gen_loss=7.364e+00, dis_loss=5.967e-02\n",
      "INFO: Iter 13600: gen_loss=7.957e+00, dis_loss=1.294e-01\n",
      "INFO: Iter 13700: gen_loss=7.569e+00, dis_loss=1.497e-01\n",
      "INFO: Iter 13800: gen_loss=7.352e+00, dis_loss=8.925e-02\n",
      "INFO: Iter 13900: gen_loss=7.008e+00, dis_loss=1.008e-01\n",
      "INFO: Iter 14000: gen_loss=7.371e+00, dis_loss=6.667e-02\n",
      "INFO: Iter 14100: gen_loss=7.868e+00, dis_loss=6.976e-02\n",
      "INFO: Iter 14200: gen_loss=8.834e+00, dis_loss=1.375e-01\n",
      "INFO: Iter 14300: gen_loss=7.287e+00, dis_loss=1.096e-01\n",
      "INFO: Iter 14400: gen_loss=7.930e+00, dis_loss=5.730e-02\n",
      "INFO: Iter 14500: gen_loss=8.840e+00, dis_loss=4.274e-02\n",
      "INFO: Iter 14600: gen_loss=8.309e+00, dis_loss=5.943e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Iter 14700: gen_loss=7.546e+00, dis_loss=2.667e-02\n",
      "INFO: Iter 14800: gen_loss=8.177e+00, dis_loss=3.690e-02\n",
      "INFO: Iter 14900: gen_loss=8.525e+00, dis_loss=9.347e-02\n",
      "INFO: Iter 15000: gen_loss=9.105e+00, dis_loss=1.056e-01\n",
      "INFO: Iter 15100: gen_loss=7.645e+00, dis_loss=1.003e-01\n",
      "INFO: Iter 15200: gen_loss=7.470e+00, dis_loss=6.760e-02\n",
      "INFO: Iter 15300: gen_loss=6.250e+00, dis_loss=2.679e-02\n",
      "INFO: Iter 15400: gen_loss=8.387e+00, dis_loss=6.504e-02\n",
      "INFO: Iter 15500: gen_loss=9.920e+00, dis_loss=8.563e-02\n",
      "INFO: Iter 15600: gen_loss=7.739e+00, dis_loss=1.464e-01\n",
      "INFO: Iter 15700: gen_loss=8.058e+00, dis_loss=1.014e-01\n",
      "INFO: Iter 15800: gen_loss=8.335e+00, dis_loss=1.140e-01\n",
      "INFO: Iter 15900: gen_loss=8.492e+00, dis_loss=6.459e-02\n",
      "INFO: Iter 16000: gen_loss=7.400e+00, dis_loss=2.310e-02\n",
      "INFO: Iter 16100: gen_loss=7.492e+00, dis_loss=3.777e-02\n",
      "INFO: Iter 16200: gen_loss=9.275e+00, dis_loss=2.565e-02\n",
      "INFO: Iter 16300: gen_loss=8.224e+00, dis_loss=1.788e-02\n",
      "INFO: Iter 16400: gen_loss=8.792e+00, dis_loss=1.517e-01\n",
      "INFO: Iter 16500: gen_loss=7.834e+00, dis_loss=1.951e-01\n",
      "INFO: Iter 16600: gen_loss=6.969e+00, dis_loss=8.368e-02\n",
      "INFO: Iter 16700: gen_loss=7.990e+00, dis_loss=1.649e-01\n",
      "INFO: Iter 16800: gen_loss=6.936e+00, dis_loss=4.710e-02\n",
      "INFO: Iter 16900: gen_loss=7.159e+00, dis_loss=9.982e-02\n",
      "INFO: Iter 17000: gen_loss=8.069e+00, dis_loss=1.142e-01\n",
      "INFO: Iter 17100: gen_loss=8.378e+00, dis_loss=6.820e-02\n",
      "INFO: Iter 17200: gen_loss=8.550e+00, dis_loss=5.604e-02\n",
      "INFO: Iter 17300: gen_loss=7.755e+00, dis_loss=5.033e-02\n",
      "INFO: Iter 17400: gen_loss=7.973e+00, dis_loss=7.686e-02\n",
      "INFO: Iter 17500: gen_loss=7.238e+00, dis_loss=1.092e-01\n",
      "INFO: Iter 17600: gen_loss=8.343e+00, dis_loss=4.282e-02\n",
      "INFO: Iter 17700: gen_loss=8.656e+00, dis_loss=3.060e-02\n",
      "INFO: Iter 17800: gen_loss=9.151e+00, dis_loss=1.805e-01\n",
      "INFO: Iter 17900: gen_loss=8.578e+00, dis_loss=2.084e-01\n",
      "INFO: Iter 18000: gen_loss=7.758e+00, dis_loss=2.590e-02\n",
      "INFO: Iter 18100: gen_loss=6.932e+00, dis_loss=5.651e-02\n",
      "INFO: Iter 18200: gen_loss=6.684e+00, dis_loss=1.065e-01\n",
      "INFO: Iter 18300: gen_loss=8.373e+00, dis_loss=7.664e-02\n",
      "INFO: Iter 18400: gen_loss=8.083e+00, dis_loss=1.452e-02\n",
      "INFO: Iter 18500: gen_loss=8.118e+00, dis_loss=1.980e-01\n",
      "INFO: Iter 18600: gen_loss=7.119e+00, dis_loss=2.367e-01\n",
      "INFO: Iter 18700: gen_loss=7.740e+00, dis_loss=1.671e-01\n",
      "INFO: Iter 18800: gen_loss=6.887e+00, dis_loss=9.407e-02\n",
      "INFO: Iter 18900: gen_loss=7.882e+00, dis_loss=7.291e-02\n",
      "INFO: Iter 19000: gen_loss=8.557e+00, dis_loss=5.758e-02\n",
      "INFO: Iter 19100: gen_loss=8.572e+00, dis_loss=8.675e-02\n",
      "INFO: Iter 19200: gen_loss=7.662e+00, dis_loss=2.479e-02\n",
      "INFO: Iter 19300: gen_loss=9.003e+00, dis_loss=5.073e-02\n",
      "INFO: Iter 19400: gen_loss=8.071e+00, dis_loss=1.441e-01\n",
      "INFO: Iter 19500: gen_loss=8.528e+00, dis_loss=3.948e-02\n",
      "INFO: Iter 19600: gen_loss=8.527e+00, dis_loss=7.956e-02\n",
      "INFO: Iter 19700: gen_loss=7.319e+00, dis_loss=1.363e-01\n",
      "INFO: Iter 19800: gen_loss=7.542e+00, dis_loss=1.219e-01\n",
      "INFO: Iter 19900: gen_loss=8.423e+00, dis_loss=3.021e-02\n",
      "INFO: Iter 20000: gen_loss=8.415e+00, dis_loss=7.720e-02\n"
     ]
    }
   ],
   "source": [
    "''' train script '''\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_iter = 0 \n",
    "max_iter = 20000\n",
    "\n",
    "report_every = 100\n",
    "save_image_every_iter = 1000 \n",
    "\n",
    "\n",
    "true_labels = torch.ones(batch_size, dtype = torch.float32, device = device)\n",
    "fake_labels = torch.zeros(batch_size, dtype = torch.float32, device = device)\n",
    "\n",
    "disc_losses = []\n",
    "gen_losses = []\n",
    "\n",
    "for batch_v in iterate_batches(envs):\n",
    "    \n",
    "    ######################## train discriminator ############################################\n",
    "    ## zero grad\n",
    "    dopt.zero_grad()\n",
    "    \n",
    "    ## prepare the inputs\n",
    "    gen_input = torch.FloatTensor(batch_size, latent_vector_size, 1,1).normal_(0,1).to(device)\n",
    "    batch_v = batch_v.to(device)\n",
    "    \n",
    "    ## forward the models \n",
    "    gen_output = gen(gen_input)\n",
    "    disc_output_on_real = disc(batch_v) \n",
    "    disc_output_on_fake = disc(gen_output.detach()) # we need only to train the disc so detach gen\n",
    "    \n",
    "    ## calculate loss \n",
    "    disc_loss = objective(disc_output_on_real, true_labels) + objective(disc_output_on_fake, fake_labels)\n",
    "    disc_losses.append(disc_loss.item())\n",
    "    \n",
    "    ## get gradients\n",
    "    disc_loss.backward()\n",
    "    ## optizer step\n",
    "    dopt.step() \n",
    "    \n",
    "    \n",
    "    ######################## train generator #################################################\n",
    "    ## zero grad \n",
    "    gopt.zero_grad()\n",
    "    \n",
    "    ## forward the model\n",
    "    disc_output_g = disc(gen_output)\n",
    "    \n",
    "    ## calcualte loss \n",
    "    gen_loss = objective(disc_output_g, true_labels) # the output should be considered as real, if not,it's a loss \n",
    "    gen_losses.append(gen_loss.item())\n",
    "    \n",
    "    ## calculate gradients\n",
    "    gen_loss.backward()\n",
    "    \n",
    "    ## optimizer step\n",
    "    gopt.step()\n",
    "    \n",
    "    \n",
    "    ################## summary writer ##########################################################\n",
    "    train_iter += 1 \n",
    "    \n",
    "    if train_iter %report_every == 0:\n",
    "        log.info(\"Iter %d: gen_loss=%.3e, dis_loss=%.3e\", train_iter, np.mean(gen_losses), np.mean(disc_losses))\n",
    "        writer.add_scalar(\"gen_loss\", np.mean(gen_losses), train_iter)\n",
    "        writer.add_scalar(\"disc_loss\", np.mean(disc_losses), train_iter)\n",
    "        gen_losses.clear()\n",
    "        disc_losses.clear()\n",
    "        \n",
    "        \n",
    "    if train_iter % save_image_every_iter == 0:\n",
    "        writer.add_image(\"fake\",vutils.make_grid(gen_output.data[:64], normalize= True), train_iter )\n",
    "        writer.add_image(\"real\", vutils.make_grid(batch_v.data[:64], normalize= True), train_iter)\n",
    "    \n",
    "    if train_iter> max_iter:\n",
    "        break \n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:04:36.293019Z",
     "start_time": "2020-04-01T14:04:36.289392Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_images(n):\n",
    "    gen.eval()\n",
    "    gen_random = torch.FloatTensor(n,latent_vector_size, 1, 1).normal_(0,1).to(device)\n",
    "    images = gen(gen_random)\n",
    "    \n",
    "    images = (images + 1)*255.0/2 \n",
    "    \n",
    "    images = images.to('cpu').detach().numpy()\n",
    "    images = np.moveaxis(images, 1, 3)\n",
    "    print(\"shape of data: \", images.shape, \" type \", type(images))\n",
    "    \n",
    "    return np.uint8(images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:17:42.318135Z",
     "start_time": "2020-04-01T14:17:42.196262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data:  (100, 64, 64, 3)  type  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "images = generate_images(100)\n",
    "for i in range(images.shape[0]):\n",
    "    cv2.imwrite('./atari saved images/GAN_generated_images/img'+str(i)+\".png\", images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:01:46.495495Z",
     "start_time": "2020-04-01T14:01:46.387310Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(gen.state_dict(), './saved_models/generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:02:19.472179Z",
     "start_time": "2020-04-01T14:02:19.409424Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(disc.state_dict(), './saved_models/discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
